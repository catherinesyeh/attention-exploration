{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d949f7e-e75f-4c73-9060-0f4e5c6ee712",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# A) load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8e089-9cc5-4512-8a2f-077fb146c4a0",
   "metadata": {},
   "source": [
    "After this step, you should end up with an array with all the sentences in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe89a278-1d55-4be1-8f58-2a59fb130253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home10/cyeh/anaconda3/envs/diffusion/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset trivia_qa (/n/home10/cyeh/.cache/huggingface/datasets/trivia_qa/rc.nocontext/1.2.0/e73c5e47a8704744fa9ded33504b35a6c098661813d1c2a09892eb9b9e9d59ae)\n",
      "100%|██████████| 3/3 [00:00<00:00, 119.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trivia_qa\", 'rc.nocontext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93bef26-19d2-4964-bc8b-0e211fccf6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17944\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset['validation']\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4bccb-9e56-4902-8304-9821ef7b93d2",
   "metadata": {},
   "source": [
    "## sample from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f07c07-bbc6-45f3-8d21-0382f2268087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for sampling\n",
    "import random\n",
    "num_examples = 250\n",
    "random.seed(10) # set seed\n",
    "indices = random.sample(range(len(test_data)), num_examples)\n",
    "indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e490348e-757f-47a3-8b24-cc3b5ee5b257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in 1968, who did radical feminist valerie solanas shoot and wound as he entered his new york studio? andy warhol',\n",
       " 'what lake can be found on the border of vermont and new york? lake champlain',\n",
       " 'which competition was won by nadiya hussain in 2015? the great british bake-off',\n",
       " 'which `b` was the name of the mechanical shark used in the original `jaws` film? bruce',\n",
       " 'who is the current (jan 2014) secretary of state for education? michael gove',\n",
       " 'which cocktail consists of rum, curacao and lime juice? mai tai',\n",
       " \"which 2001 film starring joseph fiennes and ed harris is about two opposing snipers facing each other during the battle of stalingrad? 'enemy at the gates'\",\n",
       " \"who had a 1992 hit with you're the one for me fatty? morrissey\",\n",
       " 'cassiterite is a principal ore of which metal? tin',\n",
       " 'what was the name of the band, featuring members of thin lizzy and the sex pistols, which recorded a 1979 song called ‘a merry jingle’? the greedies (originally the greedy bastards)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick 'num_examples' random Q/A pairs\n",
    "sentences = []\n",
    "sent_lengths = []\n",
    "for i in indices:\n",
    "    point = test_data[i]\n",
    "    question = point['question'].lower()\n",
    "    if question[-1] != '?': # add question mark if needed\n",
    "        question += '?'\n",
    "    answer = point['answer']['value'].lower()\n",
    "    sent = question + \" \" + answer # create sentence from q/a pair\n",
    "    sentences.append(sent) # add to list\n",
    "    sent_lengths.append(len(sent.split()))\n",
    "    \n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9857b0c-eb23-4a47-ac28-30c552c53a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "6\n",
      "15.256\n"
     ]
    }
   ],
   "source": [
    "# look at distribution of sentences\n",
    "max_len = max(sent_lengths)\n",
    "min_len = min(sent_lengths)\n",
    "mean_len = sum(sent_lengths)/len(sent_lengths)\n",
    "\n",
    "print(max_len)\n",
    "print(min_len)\n",
    "print(mean_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e4978b-1372-4a33-af2c-06af4fb0b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sentences if desired (to load back, use: np.load(\"qa_sentences.npy\"))\n",
    "import numpy as np\n",
    "np.save(\"qa_sentences.npy\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad84240-3aba-41d1-aa6f-7b07d71fb99b",
   "metadata": {},
   "source": [
    "# b) get necessary info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab40ff0-b7f0-401e-86bd-aa9cf3e08c5a",
   "metadata": {},
   "source": [
    "Here we use helper methods from Jesse Vig's bertviz repo: https://github.com/jessevig/bertviz\n",
    "* the cell below may take a while to run... this is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b6b18a-85a6-42fe-84db-ca650258de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, load back sentences\n",
    "sentences = np.load(\"qa_sentences.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ceaf87c-1de2-4c19-8ded-14b4d2cc19e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home10/cyeh/anaconda3/envs/diffusion/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import methods from bertviz\n",
    "from bertviz import neuron_view\n",
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n",
    "import string\n",
    "\n",
    "# BERT\n",
    "# model_type = 'bert'\n",
    "# model_version = 'bert-base-uncased'\n",
    "# model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)\n",
    "\n",
    "# GPT\n",
    "model_type = 'gpt2'\n",
    "model_version = 'gpt2'\n",
    "model = GPT2Model.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_version, do_lower_case=True)\n",
    "\n",
    "# may need to change depending on your model\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "\n",
    "# master dictionary for all values\n",
    "attn_dict = {'left_text': [], \n",
    "             'right_text': [], \n",
    "             'positions': [],\n",
    "             'normalized_positions': [],\n",
    "             'sentences': [],\n",
    "             'tokenized_sentences': [],\n",
    "             'q_norms': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)], \n",
    "             'k_norms': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)], \n",
    "             'queries': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)], \n",
    "             'keys': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)],\n",
    "             'attn': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)],\n",
    "             'dot_prod': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)]}\n",
    "\n",
    "for s in sentences:\n",
    "    # call method from bertviz to get attention info\n",
    "    s_dict = neuron_view.get_attention(model, model_type, tokenizer, s, include_queries_and_keys=True)['all']\n",
    "    \n",
    "    # append to master dictionary\n",
    "    tokens = s_dict['left_text']\n",
    "    attn_dict['left_text'].extend(tokens)\n",
    "    attn_dict['right_text'].extend(s_dict['right_text'])\n",
    "    \n",
    "    for index in range(len(tokens)): # save position of token and tokenized sentences too\n",
    "        attn_dict['positions'].append(index)\n",
    "        attn_dict['normalized_positions'].append(index / (len(tokens) - 1))\n",
    "        attn_dict['sentences'].append(s)\n",
    "        attn_dict['tokenized_sentences'].append(' '.join(tokens))\n",
    "        \n",
    "    for i in range(num_layers): # updating cumulative q/k vectors + attn + dp\n",
    "        for j in range(num_heads):\n",
    "            q = attn_dict['queries'][i][j]\n",
    "            k = attn_dict['keys'][i][j]\n",
    "            a = attn_dict['attn'][i][j]\n",
    "            d = attn_dict['dot_prod'][i][j]\n",
    "            q_norm = attn_dict['q_norms'][i][j]\n",
    "            k_norm = attn_dict['k_norms'][i][j]\n",
    "            \n",
    "            if len(q) == 1: # on first round, need to empty list (random string was placeholder)\n",
    "                q.clear()\n",
    "            query = s_dict['queries'][i][j]\n",
    "            q.extend(query)\n",
    "            np_query = np.array(query)\n",
    "\n",
    "            if len(k) == 1:\n",
    "                k.clear()\n",
    "            key = s_dict['keys'][i][j]\n",
    "            k.extend(key)\n",
    "            np_key = np.array(key)\n",
    "            \n",
    "            if len(a) == 1:\n",
    "                a.clear()\n",
    "            a.extend(s_dict['attn'][i][j])\n",
    "            \n",
    "            if len(d) == 1:\n",
    "                d.clear()\n",
    "            dp = np.dot(np_query, np_key.transpose())\n",
    "            d.extend(dp)\n",
    "            \n",
    "            # norms too\n",
    "            if len(q_norm) == 1:\n",
    "                q_norm.clear()\n",
    "            if len(k_norm) == 1:\n",
    "                k_norm.clear()\n",
    "            sent_q_norms = [np.linalg.norm(q)for q in np_query]\n",
    "            q_norm.extend(sent_q_norms)\n",
    "            sent_k_norms = [np.linalg.norm(k)for k in np_key]\n",
    "            k_norm.extend(sent_k_norms)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada60c1-263a-41fb-828f-16da84942720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional if you want to save this dictionary (to load back, use: np.load(\"attn_dict.pkl.npy\", allow_pickle=True).item()\n",
    "np.save(\"attn_dict.pkl\", attn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a681e0-c1a0-43ed-b898-d3ffa08e6b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data = False # keep false unless you want to crop data further (e.g., see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b907d4-bf84-41af-9db6-c52c7e2ad805",
   "metadata": {
    "tags": []
   },
   "source": [
    "### optional (crop the data further)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0628ae9-eacb-485e-88ab-657e749e288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'turkey', '[SEP]', '[CLS]', 'which', 'famous', 'jewel', '##lers', 'makes', 'the', 'super', 'bowl', 'trophy', '?', 'tiffany', '&', 'co', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# bert\n",
    "print(attn_dict[\"left_text\"][5000:5019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e72f3a74-f835-4e54-bbd6-e6fd64723476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'which famous jewellers makes the super bowl trophy? tiffany & co.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bert\n",
    "sentences[226]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e912d1af-313a-4864-aab9-2879b60a6cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'igel', ' haw', 'th', 'orne', ' was', ' o', 'scar', ' nominated', ' for', ' the', ' madness', ' of', ' which', ' king', '?', ' ge', 'orge']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nigel hawthorne was oscar nominated for the madness of which king? george'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt\n",
    "print(attn_dict[\"left_text\"][5002:5020])\n",
    "sentences[223]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350ed192-f182-4440-98a6-bfb02d113d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b504d-68ea-4612-8c92-6be39a5f5d52",
   "metadata": {
    "tags": []
   },
   "source": [
    "# c) generate tokens.json file (1 file w/ info shared across all attn heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8551c2-3d69-465c-aa5a-ed6a91ac66e0",
   "metadata": {},
   "source": [
    "Format of this file:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"tokens\": [\n",
    "    {\n",
    "      \"value\": \"[cls]\",\n",
    "      \"type\": \"query\",\n",
    "      \"pos_int\": 0,\n",
    "      \"length\": 13,\n",
    "      \"position\": 0.0,\n",
    "      \"sentence\": \"[CLS] synth ##pop band freeze ##pop have used it on stage . [SEP]\"\n",
    "    },\n",
    "    ...,\n",
    "    {\n",
    "      \"value\": \"synth\",\n",
    "      \"type\": \"query\",\n",
    "      \"pos_int\": 1,\n",
    "      \"length\": 13,\n",
    "      \"position\": 0.083333,\n",
    "      \"sentence\": \"[CLS] synth ##pop band freeze ##pop have used it on stage . [SEP]\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "* **value:** the token\n",
    "* **type:** query or key\n",
    "* **pos_int:** position of token in sentence (zero index)\n",
    "* **length:** length of sentence\n",
    "* **position:** normalized position in sentence (i.e., pos_int / length - 1)\n",
    "* **sentence:** full sentence the current token came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f1718f9-b2e6-470c-b843-88fc90645238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/gpt/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make folder for data if doesn't already exist\n",
    "import os\n",
    "\n",
    "# outer data folder\n",
    "outer_data = \"data/\"\n",
    "if not os.path.exists(outer_data):\n",
    "    os.mkdir(outer_data)\n",
    "  \n",
    "# folder for this model\n",
    "model_name = \"gpt\" if \"gpt\" in model_type else model_type\n",
    "data_folder = outer_data + model_name + \"/\"\n",
    "if not os.path.exists(data_folder):\n",
    "    os.mkdir(data_folder)\n",
    "    \n",
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f1e92b-2457-47de-9c2d-36afa72fdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info out of attn_dict\n",
    "tokens = attn_dict['left_text']\n",
    "positions = attn_dict['positions']\n",
    "norm_pos = attn_dict['normalized_positions']\n",
    "tok_sentences = attn_dict['tokenized_sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c9fce78-a447-4fd4-adea-6c54ac223d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5020"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cutoff = 5020 if crop_data else len(tokens)\n",
    "token_cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fbf5c4-d678-409d-bdb9-8381f8356dde",
   "metadata": {},
   "source": [
    "## actually formatting into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79e24539-bbb7-49af-a209-9d2147c13d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f0e50d-3858-4f24-89d2-4a5f02490d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer json object\n",
    "shared_json = {\"tokens\": []}\n",
    "\n",
    "# repeat for queries and keys\n",
    "for tok_type in [\"query\", \"key\"]:\n",
    "    for i in range(token_cutoff):\n",
    "        # create new dictionary for each token\n",
    "        new_token = {}\n",
    "        \n",
    "        # reformat sentence (gpt output has extra spaces for some reason)\n",
    "        sent = tok_sentences[i]\n",
    "        split_sent = sent.split()\n",
    "        sent_len = len(split_sent);\n",
    "        sent_format = ' '.join(split_sent)\n",
    "        \n",
    "        # fill in info\n",
    "        new_token[\"value\"] = tokens[i].strip()\n",
    "        new_token[\"type\"] = tok_type\n",
    "        new_token[\"pos_int\"] = int(positions[i])\n",
    "        new_token[\"length\"] = sent_len\n",
    "        new_token[\"position\"] = round(float(norm_pos[i]), 3)\n",
    "        new_token[\"sentence\"] = sent_format\n",
    "\n",
    "        shared_json[\"tokens\"].append(new_token)\n",
    "\n",
    "# save to json file\n",
    "json_str = json.dumps(shared_json)\n",
    "with open(data_folder + \"tokens.json\", \"w\") as f:\n",
    "    f.write(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc699dc-bf41-4c43-82fe-b58a68ce7cad",
   "metadata": {},
   "source": [
    "# d) attention files (one for each attention head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7e78f-54ce-4b0a-9b30-d59feaeb1043",
   "metadata": {},
   "source": [
    "Format of these files:\n",
    "```\n",
    "{\n",
    "    \"layer\": 0, \n",
    "     \"head\": 0, \n",
    "     \"tokens\": [\n",
    "        {\n",
    "            \"attention\": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        }, \n",
    "        ...,\n",
    "        {\n",
    "            \"attention\": [0.772724, 0.227276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        }\n",
    "     ]\n",
    "}\n",
    "```   \n",
    "* store **attention weights** for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e7fd205-e4ea-45e5-97d9-41cdf3a0532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make attention data subfolder\n",
    "attn_folder = data_folder + \"attention/\"\n",
    "\n",
    "if not os.path.exists(attn_folder):\n",
    "    os.mkdir(attn_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65176bda-5c8c-487e-9929-502d45849fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attention info\n",
    "attention = attn_dict['attn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4021c6f-db0a-4b9a-92d1-e9bae10d8e69",
   "metadata": {},
   "source": [
    "## only for gpt (value norm preprocessing -- for sentence attention visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77ee7cf1-2559-4996-bb2d-238a4cd65cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dict\n",
    "value_norms = {}\n",
    "for head in range(12):\n",
    "    for layer in range(12):\n",
    "        value_norms[(head, layer)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e818c93c-3493-4c31-b5bd-3af866700959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get value norms\n",
    "\n",
    "# GPT\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "from numpy.linalg import norm\n",
    "\n",
    "model_type = 'gpt2'\n",
    "model_version = 'gpt2'\n",
    "model = GPT2Model.from_pretrained(model_version, return_dict=True, output_attentions=True, output_hidden_states=True, use_cache=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_version, do_lower_case=True)\n",
    "\n",
    "for sent in sentences:\n",
    "    inputs = tokenizer(sent, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    key_values = outputs.past_key_values\n",
    "    num_tokens = len(key_values[0][0][0][0])\n",
    "   \n",
    "    for head in range(12):\n",
    "        for layer in range(12):\n",
    "            head_norms = []\n",
    "            for i in range(num_tokens):\n",
    "                val = key_values[head][1][0][layer][i].detach().numpy()\n",
    "                val_norm = norm(val)\n",
    "                head_norms.append(val_norm)\n",
    "            value_norms[(head, layer)].extend(head_norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc2050-44af-4de1-8d9e-1f9b9baa7d37",
   "metadata": {},
   "source": [
    "## now make attention files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "965106be-47a6-4b81-9624-da8094278e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get attention for keys\n",
    "def k_matrix(q_matrix):\n",
    "    # assumes for specific layer + head (e.g., queries[0][0])\n",
    "    num_tokens = len(q_matrix)\n",
    "    k_matrix = []\n",
    "    i = 0\n",
    "    while i < token_cutoff:\n",
    "        q = q_matrix[i]\n",
    "        sent_length = len(q)\n",
    "        for k_i in range(sent_length):\n",
    "            k = []\n",
    "            for q_i in range(sent_length):\n",
    "                k.append(q_matrix[q_i + i][k_i])\n",
    "            k_matrix.append(k)\n",
    "        i += sent_length\n",
    "    \n",
    "    return k_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "369f2932-5057-4f37-9506-8f19ec4792ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json file for each layer/head combo\n",
    "for layer in range(num_layers): # updating cumulative q/k vectors + attn + dp\n",
    "    for head in range(num_heads):\n",
    "        new_json = {}\n",
    "        new_json[\"layer\"] = layer\n",
    "        new_json[\"head\"] = head\n",
    "        new_json[\"tokens\"] = []\n",
    "        \n",
    "        # COMMENT OUT BELOW IF USING BERT\n",
    "        head_val_norms = value_norms[(layer, head)]\n",
    "        \n",
    "        for tok_type in [\"query\", \"key\"]:\n",
    "            head_attn = attention[layer][head]\n",
    "            if tok_type == \"key\":\n",
    "                # transpose attn vals if key\n",
    "                head_attn = k_matrix(head_attn)\n",
    "                \n",
    "            for i in range(token_cutoff):\n",
    "                # iterate through all tokens\n",
    "                new_token = {}\n",
    "                attn = head_attn[i]\n",
    "                # format attention vals first\n",
    "                attn_format = [round(float(a), 3) for a in attn]\n",
    "                new_token[\"attention\"] = attn_format\n",
    "                # COMMENT OUT BELOW IF USING BERT\n",
    "                new_token[\"value_norm\"] = round(float(head_val_norms[i]), 3)\n",
    "                \n",
    "                new_json[\"tokens\"].append(new_token)\n",
    "            \n",
    "        json_str = json.dumps(new_json)\n",
    "        with open(attn_folder + \"layer{}_head{}.json\".format(layer, head), \"w\") as f:\n",
    "            f.write(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b810661-98a3-4b6c-8a0d-9f46b85742cf",
   "metadata": {},
   "source": [
    "# e) TSNE/UMAP/PCA helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10784883-8e8a-4781-9388-57b4c8872157",
   "metadata": {},
   "source": [
    "Will be used in next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ef0629d-681d-4e67-b27f-17966a52201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5adb5a58-7005-4260-9c23-52f3adc62d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA\n",
    "def run_pca(data, dim):\n",
    "    if dim == 3:\n",
    "        pca = PCA(n_components=3) # 3D\n",
    "    else:\n",
    "        pca = PCA(n_components=2) # 2D\n",
    "    pca_results = pca.fit_transform(data)\n",
    "    return pca_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4777ac76-6d84-453c-9a14-95d230e8c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run TSNE\n",
    "def run_tsne(data, dim):\n",
    "    if dim == 3: # 3D\n",
    "        tsne = TSNE(n_components=3, verbose=0, perplexity=100, n_iter=300, metric=\"cosine\", n_jobs=-1) \n",
    "    else: # 2D\n",
    "        tsne = TSNE(n_components=2, verbose=0, perplexity=100, n_iter=300, metric=\"cosine\", n_jobs=-1)\n",
    "    tsne_results = tsne.fit_transform(data)\n",
    "    return tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41dbf4f1-9200-4627-b22e-743b08196471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run UMAP\n",
    "def run_umap(data, dim):\n",
    "    time_start = time.time()\n",
    "    if dim == 3: # 3D\n",
    "        umap = UMAP(n_components=3, init='random', random_state=0, metric='cosine')\n",
    "    else: # 2D\n",
    "        umap = UMAP(n_components=2, init='random', random_state=0, metric='cosine')\n",
    "    umap_results = umap.fit_transform(data)\n",
    "    return umap_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28634dd2-1c48-4ee7-94b8-821ebf7e6a3c",
   "metadata": {},
   "source": [
    "# f) byLayerHead files (one for each attention head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74bc5a-c11e-41aa-a9df-fc1d4e6e9df3",
   "metadata": {},
   "source": [
    "Format of these files:\n",
    "```\n",
    "{\n",
    "    \"layer\": 0, \n",
    "     \"head\": 0, \n",
    "     \"tokens\": [\n",
    "         {\n",
    "             \"tsne_x\": 6.876875, \n",
    "             \"tsne_y\": 7.453007, \n",
    "             \"umap_x\": -11.346616, \n",
    "             \"umap_y\": 12.795157, \n",
    "             \"norm\": 5.628239, \n",
    "             \"tsne_x_3d\": -8.263657, \n",
    "             \"tsne_y_3d\": 0.388844,\n",
    "             \"tsne_z_3d\": -1.241852, \n",
    "             \"umap_x_3d\": 17.736557, \n",
    "             \"umap_y_3d\": -3.88329, \n",
    "             \"umap_z_3d\": 2.701952, \n",
    "             \"pca_x\": -2.444411, \n",
    "             \"pca_y\": -0.763636, \n",
    "             \"pca_x_3d\": -2.444411, \n",
    "             \"pca_y_3d\": -0.763568, \n",
    "             \"pca_z_3d\": 2.882317\n",
    "         }, \n",
    "         ...,\n",
    "         {\n",
    "             ...\n",
    "         }\n",
    "     ]\n",
    "}\n",
    "```   \n",
    "* includes **2D/3D** **TSNE/UMAP/PCA** coordinates for each token for current layer and head + **norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26225324-39c3-48af-bf35-0d7a8edbd427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info from attn_dict\n",
    "queries = attn_dict[\"queries\"]\n",
    "keys = attn_dict[\"keys\"]\n",
    "q_norms = attn_dict[\"q_norms\"]\n",
    "k_norms = attn_dict[\"k_norms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "886466ae-65f4-4a07-ad31-8321b5412eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new folder for byLayerHead files\n",
    "coord_folder = data_folder + \"byLayerHead/\"\n",
    "\n",
    "if not os.path.exists(coord_folder):\n",
    "    os.mkdir(coord_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33dd4c9-7bf3-486a-ad9d-5998aba19e7a",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0560b70-640a-49da-b166-bf59949a4b71",
   "metadata": {},
   "source": [
    "Here, I translate the keys to have same centroid as the queries (for better visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0af0bfe-5d62-4880-a570-e0e687ee3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get means\n",
    "mean_queries = []\n",
    "mean_keys = []\n",
    "for layer in range(num_layers):\n",
    "    for head in range(num_heads):\n",
    "        q = queries[layer][head][:token_cutoff]\n",
    "        k = keys[layer][head][:token_cutoff]\n",
    "        num_tokens = len(q)\n",
    "        mean_q = [0] * 64\n",
    "        mean_k = [0] * 64\n",
    "        for i in range(num_tokens):\n",
    "            for j in range(64):\n",
    "                q_token = q[i]\n",
    "                k_token = k[i]\n",
    "                mean_q[j] += q_token[j]\n",
    "                mean_k[j] += k_token[j]\n",
    "        \n",
    "        mean_q = [i / num_tokens for i in mean_q]\n",
    "        mean_k = [i / num_tokens for i in mean_k]\n",
    "        mean_queries.append(mean_q)\n",
    "        mean_keys.append(mean_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e1561ed-62d1-4cd2-99f7-e5e42ff2d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix key means\n",
    "new_mean_keys = []\n",
    "for layer in range(12):\n",
    "    for head in range(12):\n",
    "        mean_k = [0] * 64\n",
    "        k = keys[layer][head][:token_cutoff]\n",
    "        m_k = mean_keys[12 * layer + head]\n",
    "        q_k = mean_queries[12 * layer + head]\n",
    "        num_tokens = len(k)\n",
    "        for i in range(num_tokens):\n",
    "            k_token = k[i]\n",
    "            for j in range(64):\n",
    "                orig_k = k_token[j]\n",
    "                k_token[j] = orig_k - m_k[j] + q_k[j]  \n",
    "                mean_k[j] += k_token[j]\n",
    "        \n",
    "        mean_k = [i / num_tokens for i in mean_k]\n",
    "        new_mean_keys.append(mean_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890da6f-fb97-4957-a2f3-bbc8ba9dec4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### optional: sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d8da3c2-1459-4ffe-9f7e-4a6b260afffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6328304488842647, -0.3406692687978604, 0.19263284422352397, -0.29356196707122, -0.3865076022601104, 0.018325021613456043, -0.5140582495409832, -0.22270956292017494, -0.1028417951616633, -0.10767029772950448, -0.19459884084512544, -0.16332341774470688, -0.31990569537631663, 0.14333667343786632, 0.02661175984106364, -0.20699877709083525, 0.01230896841901618, 0.1584361630165393, -0.42631362929079397, -0.37456994655197, 0.34503850432194444, 0.04248234130167569, -0.009814457149352447, 0.16992524190688973, -0.042799583158596784, -0.5034576060632577, -0.4461871687051992, -0.24721287545224882, 0.21120566658697007, -0.2244664302707995, 0.09846800615699346, -0.014856036968503251, 0.31801266142044704, -0.27427087990187693, -0.6178917641551104, -0.08074602001008997, 0.3457698167524341, -0.19586435282784717, 0.330209734710861, -0.09761096816472857, 0.28747954536340686, -0.4640231773598617, -0.22340672463939276, -0.29028919925505065, 0.18114175643953698, -0.19915520402248504, 0.26762874493917144, 0.6854982039746476, 0.48184828933929486, -1.0067578486482283, -0.007181147642366856, -0.35888122106935694, -0.036871688592756, -0.6814352080769526, -0.05984506915171308, -0.5772989736621331, -0.10837003956233482, -0.12248936401294225, -0.1301933474058612, -0.26240676964440224, 0.75825008351571, -0.3461413824038424, 0.6597891669345656, -0.007641511881701737]\n",
      "[-2.293006557738594, 2.2385373591196784, 1.864213205847993, 0.420336482306178, 1.0997030671934533, 0.37983392998056703, 0.9504727178750434, 0.4905618796830967, -0.8898111485681345, 0.17945298559043513, 0.03153513523831637, 0.6653832244669158, 0.6201295511448288, 0.12326301738899943, -0.5422204151807123, 0.813863840187154, 0.1800364144780929, -0.9026757037020344, 1.7966417612136893, -0.3366744066858516, -2.02376371203547, -0.4769822094821375, -0.42820042634894884, -1.1632023745260955, -0.1861803129375671, 0.27006093358720257, -0.01147634593836696, 0.05381720818716484, -0.6570627174056339, -0.6139837848486386, -1.0757729368853899, -1.2441774389038671, -2.5620409724855446, 0.6958820027800136, 1.683302594291462, -0.03175845883547354, 1.1253482847945742, 1.2213327744382014, 0.6877369274580072, 0.043366363287654766, -0.254380561342923, 1.0134126617167836, -0.1409270518644902, 0.9513941211872678, -0.575559652323623, -0.6584887103097833, 1.2313024798360843, -0.6968015876127429, -0.6503925456149704, 1.207953217369226, 0.8993894203701446, -1.2467506868230454, 2.0229617526769577, 0.22971253515447515, -0.34648067226143653, 2.116459447848963, 0.425609498230028, 0.09573623968234501, 0.8602277497799912, 1.328729982177664, -1.4927650951086524, -1.0296778457513844, -1.7341865787306705, 2.037478306252821]\n"
     ]
    }
   ],
   "source": [
    "print(mean_queries[0])\n",
    "print(mean_keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "917cfacd-5b37-47b0-9508-6eadada6be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6328304488842397, -0.34066926879785087, 0.1926328442235162, -0.29356196707124405, -0.3865076022601452, 0.018325021613455575, -0.5140582495410151, -0.22270956292017352, -0.10284179516167383, -0.1076702977295075, -0.1945988408451306, -0.1633234177446997, -0.31990569537629565, 0.1433366734378739, 0.026611759841065195, -0.20699877709084324, 0.012308968419017255, 0.1584361630165289, -0.42631362929077987, -0.3745699465520047, 0.3450385043219235, 0.04248234130167755, -0.009814457149351901, 0.16992524190689354, -0.0427995831585904, -0.5034576060632645, -0.44618716870521064, -0.24721287545224752, 0.21120566658696158, -0.22446643027079363, 0.09846800615698863, -0.01485603696850189, 0.31801266142046836, -0.27427087990189397, -0.617891764155083, -0.0807460200100868, 0.3457698167523939, -0.19586435282783596, 0.33020973471088444, -0.09761096816473555, 0.2874795453633768, -0.4640231773598779, -0.22340672463938815, -0.2902891992550295, 0.18114175643955463, -0.19915520402248896, 0.2676287449391595, 0.6854982039745966, 0.4818482893392777, -1.0067578486481852, -0.007181147642367371, -0.35888122106932197, -0.03687168859275869, -0.6814352080770121, -0.05984506915171172, -0.5772989736621545, -0.10837003956234544, -0.12248936401292522, -0.1301933474058555, -0.262406769644397, 0.7582500835158088, -0.34614138240384756, 0.6597891669345379, -0.007641511881702061]\n"
     ]
    }
   ],
   "source": [
    "print(new_mean_keys[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36636b8-c1b4-4703-be25-610d9a561931",
   "metadata": {},
   "source": [
    "## get the json files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe9b02-94f1-48e0-a2f0-5f8f1bafc3be",
   "metadata": {},
   "source": [
    "the code block below will likely take some time to run too (I usually start multiple cluster sessions to finish faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4bdb47b-c0a8-47a0-a9db-753b63959cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 Head 0 complete: 306.3622417449951\n",
      "Layer 0 Head 1 complete: 148.55165147781372\n",
      "Layer 0 Head 2 complete: 123.72091770172119\n",
      "Layer 0 Head 3 complete: 122.75478434562683\n",
      "Layer 0 Head 4 complete: 149.74448490142822\n",
      "Layer 0 Head 5 complete: 112.99003767967224\n",
      "Layer 0 Head 6 complete: 106.97444701194763\n",
      "Layer 0 Head 7 complete: 149.20293641090393\n",
      "Layer 0 Head 8 complete: 142.5443639755249\n",
      "Layer 0 Head 9 complete: 109.06266856193542\n",
      "Layer 0 Head 10 complete: 122.44500541687012\n",
      "Layer 0 Head 11 complete: 123.2271318435669\n",
      "Layer 1 Head 0 complete: 110.57423138618469\n",
      "Layer 1 Head 1 complete: 99.39705801010132\n",
      "Layer 1 Head 2 complete: 102.20657849311829\n",
      "Layer 1 Head 3 complete: 106.1483371257782\n",
      "Layer 1 Head 4 complete: 104.62596726417542\n",
      "Layer 1 Head 5 complete: 111.96645045280457\n",
      "Layer 1 Head 6 complete: 123.50146627426147\n",
      "Layer 1 Head 7 complete: 163.91066360473633\n",
      "Layer 1 Head 8 complete: 115.58942008018494\n",
      "Layer 1 Head 9 complete: 107.74760293960571\n",
      "Layer 1 Head 10 complete: 107.50282883644104\n",
      "Layer 1 Head 11 complete: 119.43935918807983\n",
      "Layer 2 Head 0 complete: 132.16502046585083\n",
      "Layer 2 Head 1 complete: 130.69354057312012\n",
      "Layer 2 Head 2 complete: 122.67850303649902\n",
      "Layer 2 Head 3 complete: 117.73439168930054\n",
      "Layer 2 Head 4 complete: 145.9699821472168\n",
      "Layer 2 Head 5 complete: 123.21667623519897\n",
      "Layer 2 Head 6 complete: 111.06069588661194\n",
      "Layer 2 Head 7 complete: 99.25733375549316\n",
      "Layer 2 Head 8 complete: 119.74567699432373\n",
      "Layer 2 Head 9 complete: 117.53419947624207\n",
      "Layer 2 Head 10 complete: 106.43362188339233\n",
      "Layer 2 Head 11 complete: 114.44927215576172\n",
      "Layer 3 Head 0 complete: 138.92870354652405\n",
      "Layer 3 Head 1 complete: 108.35380291938782\n",
      "Layer 3 Head 2 complete: 120.35132241249084\n",
      "Layer 3 Head 3 complete: 130.013254404068\n",
      "Layer 3 Head 4 complete: 125.08871221542358\n",
      "Layer 3 Head 5 complete: 113.09731245040894\n",
      "Layer 3 Head 6 complete: 122.76127362251282\n",
      "Layer 3 Head 7 complete: 121.95677447319031\n",
      "Layer 3 Head 8 complete: 121.4539406299591\n",
      "Layer 3 Head 9 complete: 109.64194297790527\n",
      "Layer 3 Head 10 complete: 114.26253938674927\n",
      "Layer 3 Head 11 complete: 112.5662624835968\n",
      "Layer 4 Head 0 complete: 115.38553142547607\n",
      "Layer 4 Head 1 complete: 120.0520281791687\n",
      "Layer 4 Head 2 complete: 112.61602568626404\n",
      "Layer 4 Head 3 complete: 121.5017306804657\n",
      "Layer 4 Head 4 complete: 114.82732439041138\n",
      "Layer 4 Head 5 complete: 132.6568808555603\n",
      "Layer 4 Head 6 complete: 99.2817223072052\n",
      "Layer 4 Head 7 complete: 115.8139877319336\n",
      "Layer 4 Head 8 complete: 135.44391989707947\n",
      "Layer 4 Head 9 complete: 103.1111216545105\n",
      "Layer 4 Head 10 complete: 131.79685974121094\n",
      "Layer 4 Head 11 complete: 123.35872411727905\n",
      "Layer 5 Head 0 complete: 153.7478494644165\n",
      "Layer 5 Head 1 complete: 131.04042673110962\n",
      "Layer 5 Head 2 complete: 90.52506995201111\n",
      "Layer 5 Head 3 complete: 106.767991065979\n",
      "Layer 5 Head 4 complete: 136.81048345565796\n",
      "Layer 5 Head 5 complete: 127.88467121124268\n",
      "Layer 5 Head 6 complete: 125.81821608543396\n",
      "Layer 5 Head 7 complete: 117.66429829597473\n",
      "Layer 5 Head 8 complete: 124.38968515396118\n",
      "Layer 5 Head 9 complete: 143.9554328918457\n",
      "Layer 5 Head 10 complete: 107.1864025592804\n",
      "Layer 5 Head 11 complete: 146.5139570236206\n",
      "Layer 6 Head 0 complete: 107.78699970245361\n",
      "Layer 6 Head 1 complete: 97.9130470752716\n",
      "Layer 6 Head 2 complete: 153.6989233493805\n",
      "Layer 6 Head 3 complete: 107.2784640789032\n",
      "Layer 6 Head 4 complete: 197.49727749824524\n",
      "Layer 6 Head 5 complete: 124.29113411903381\n",
      "Layer 6 Head 6 complete: 149.9970736503601\n",
      "Layer 6 Head 7 complete: 107.84285402297974\n",
      "Layer 6 Head 8 complete: 116.77489924430847\n",
      "Layer 6 Head 9 complete: 130.93438267707825\n",
      "Layer 6 Head 10 complete: 122.84235620498657\n",
      "Layer 6 Head 11 complete: 112.21176433563232\n",
      "Layer 7 Head 0 complete: 107.17579412460327\n",
      "Layer 7 Head 1 complete: 96.17026948928833\n",
      "Layer 7 Head 2 complete: 141.79198265075684\n",
      "Layer 7 Head 3 complete: 102.66574740409851\n",
      "Layer 7 Head 4 complete: 114.56610703468323\n",
      "Layer 7 Head 5 complete: 140.83877682685852\n",
      "Layer 7 Head 6 complete: 107.07576608657837\n",
      "Layer 7 Head 7 complete: 125.04684615135193\n",
      "Layer 7 Head 8 complete: 98.60583472251892\n",
      "Layer 7 Head 9 complete: 117.49774289131165\n",
      "Layer 7 Head 10 complete: 140.13346362113953\n",
      "Layer 7 Head 11 complete: 107.16454577445984\n",
      "Layer 8 Head 0 complete: 109.20439791679382\n",
      "Layer 8 Head 1 complete: 143.25545859336853\n",
      "Layer 8 Head 2 complete: 134.16751646995544\n",
      "Layer 8 Head 3 complete: 141.18673133850098\n",
      "Layer 8 Head 4 complete: 126.27290034294128\n",
      "Layer 8 Head 5 complete: 133.09649109840393\n",
      "Layer 8 Head 6 complete: 122.96094965934753\n",
      "Layer 8 Head 7 complete: 115.96435308456421\n",
      "Layer 8 Head 8 complete: 136.04573154449463\n",
      "Layer 8 Head 9 complete: 181.29923677444458\n",
      "Layer 8 Head 10 complete: 124.15776348114014\n",
      "Layer 8 Head 11 complete: 166.99101734161377\n",
      "Layer 9 Head 0 complete: 125.10933327674866\n",
      "Layer 9 Head 1 complete: 105.58466625213623\n",
      "Layer 9 Head 2 complete: 156.0762176513672\n",
      "Layer 9 Head 3 complete: 101.62959241867065\n",
      "Layer 9 Head 4 complete: 105.43648767471313\n",
      "Layer 9 Head 5 complete: 110.41303038597107\n",
      "Layer 9 Head 6 complete: 102.99922394752502\n",
      "Layer 9 Head 7 complete: 200.00345611572266\n",
      "Layer 9 Head 8 complete: 137.41168355941772\n",
      "Layer 9 Head 9 complete: 104.13682174682617\n",
      "Layer 9 Head 10 complete: 123.41746926307678\n",
      "Layer 9 Head 11 complete: 120.28478789329529\n",
      "Layer 10 Head 0 complete: 141.55614256858826\n",
      "Layer 10 Head 1 complete: 105.40546011924744\n",
      "Layer 10 Head 2 complete: 136.78640818595886\n",
      "Layer 10 Head 3 complete: 95.44680213928223\n",
      "Layer 10 Head 4 complete: 131.83271026611328\n",
      "Layer 10 Head 5 complete: 108.68349814414978\n",
      "Layer 10 Head 6 complete: 124.26738953590393\n",
      "Layer 10 Head 7 complete: 116.97145128250122\n",
      "Layer 10 Head 8 complete: 108.86869192123413\n",
      "Layer 10 Head 9 complete: 121.3265151977539\n",
      "Layer 10 Head 10 complete: 100.04934048652649\n",
      "Layer 10 Head 11 complete: 133.78566360473633\n",
      "Layer 11 Head 0 complete: 104.05474257469177\n",
      "Layer 11 Head 1 complete: 207.31497859954834\n",
      "Layer 11 Head 2 complete: 140.04041576385498\n",
      "Layer 11 Head 3 complete: 154.6377604007721\n",
      "Layer 11 Head 4 complete: 112.99027514457703\n",
      "Layer 11 Head 5 complete: 105.2123875617981\n",
      "Layer 11 Head 6 complete: 111.66277503967285\n",
      "Layer 11 Head 7 complete: 276.9671437740326\n",
      "Layer 11 Head 8 complete: 117.47823524475098\n",
      "Layer 11 Head 9 complete: 103.16706538200378\n",
      "Layer 11 Head 10 complete: 121.7327070236206\n",
      "Layer 11 Head 11 complete: 184.80794858932495\n"
     ]
    }
   ],
   "source": [
    "# create json file for each layer/head combo \n",
    "for layer in range(num_layers): # updating cumulative q/k vectors + attn + dp\n",
    "    for head in range(num_heads):\n",
    "        time_start = time.time()\n",
    "        new_json = {}\n",
    "        new_json[\"layer\"] = layer\n",
    "        new_json[\"head\"] = head\n",
    "        new_json[\"tokens\"] = []\n",
    "        \n",
    "        head_queries = queries[layer][head][:token_cutoff]\n",
    "        head_keys = keys[layer][head][:token_cutoff]\n",
    "        head_data = head_queries + head_keys\n",
    "        head_data = np.array(head_data)\n",
    "        \n",
    "        head_norms = q_norms[layer][head][:token_cutoff] + k_norms[layer][head][:token_cutoff]\n",
    "        total_tokens = token_cutoff * 2\n",
    "        \n",
    "        # run TSNE/UMAP/PCA\n",
    "        tsne = run_tsne(head_data, 2)\n",
    "        tsne_3d = run_tsne(head_data, 3)\n",
    "        umap = run_umap(head_data, 2)\n",
    "        umap_3d = run_umap(head_data, 3)\n",
    "        pca = run_pca(head_data, 2)\n",
    "        pca_3d = run_pca(head_data, 3)\n",
    "                \n",
    "        for i in range(total_tokens):\n",
    "            # iterate through all tokens\n",
    "            new_token = {}\n",
    "            new_token[\"tsne_x\"] = round(float(tsne[i][0]), 3)\n",
    "            new_token[\"tsne_y\"] = round(float(tsne[i][1]), 3)\n",
    "            new_token[\"umap_x\"] = round(float(umap[i][0]), 3)\n",
    "            new_token[\"umap_y\"] = round(float(umap[i][1]), 3)\n",
    "            new_token[\"norm\"] = round(float(head_norms[i]), 3)\n",
    "            new_token[\"tsne_x_3d\"] = round(float(tsne_3d[i][0]), 3)\n",
    "            new_token[\"tsne_y_3d\"] = round(float(tsne_3d[i][1]), 3)\n",
    "            new_token[\"tsne_z_3d\"] = round(float(tsne_3d[i][2]), 3)\n",
    "            new_token[\"umap_x_3d\"] = round(float(umap_3d[i][0]), 3)\n",
    "            new_token[\"umap_y_3d\"] = round(float(umap_3d[i][1]), 3)\n",
    "            new_token[\"umap_z_3d\"] = round(float(umap_3d[i][2]), 3)\n",
    "            new_token[\"pca_x\"] = round(float(pca[i][0]), 3)\n",
    "            new_token[\"pca_y\"] = round(float(pca[i][1]), 3)\n",
    "            new_token[\"pca_x_3d\"] = round(float(pca_3d[i][0]), 3)\n",
    "            new_token[\"pca_y_3d\"] = round(float(pca_3d[i][1]), 3)\n",
    "            new_token[\"pca_z_3d\"] = round(float(pca_3d[i][2]), 3)\n",
    "            \n",
    "            new_json[\"tokens\"].append(new_token)\n",
    "            \n",
    "        json_str = json.dumps(new_json)\n",
    "        with open(coord_folder + \"layer{}_head{}.json\".format(layer, head), \"w\") as f:\n",
    "            f.write(json_str)\n",
    "            \n",
    "        time_elapsed = time.time() - time_start\n",
    "        print(\"Layer {} Head {} complete: {}\".format(layer, head, time_elapsed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ad856-b768-4387-8587-4424255058a2",
   "metadata": {},
   "source": [
    "# g) You're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b275f6-82d7-4159-90a8-18fb81a623e1",
   "metadata": {},
   "source": [
    "download all the data files & place accordingly (inside web/**data**/ folder) to use in our attention visualization tool: https://github.com/catherinesyeh/attention-viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0aee90-e7ee-4437-9ed8-0758ef3e23b6",
   "metadata": {},
   "source": [
    "# optional: shrink data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30a505e5-8129-4632-b3c0-ce408908e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_folder = \"data/gpt/\"\n",
    "bert_folder = \"data/bert/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3b62d-8562-409b-a72d-7ff93b11c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token files\n",
    "for folder in [gpt_folder, bert_folder]:\n",
    "    token_file = folder + \"tokens.json\"\n",
    "    data = json.load(open(token_file, 'r'))\n",
    "    \n",
    "    tokens = data['tokens']\n",
    "    for i in range(len(tokens)):\n",
    "        t = tokens[i]\n",
    "        t[\"position\"] = round(float(t[\"position\"]), 3)\n",
    "        \n",
    "    json_str = json.dumps(data)\n",
    "    with open(token_file, \"w\") as o:\n",
    "        o.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28a4f92e-5889-40bd-aced-e0f30cb60338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention files\n",
    "for folder in [gpt_folder, bert_folder]:\n",
    "    att_folder = folder + \"attention/\"\n",
    "    for json_file in sorted(os.listdir(att_folder)):\n",
    "        if \".json\" not in json_file:\n",
    "            continue\n",
    "        f = att_folder + json_file\n",
    "        data = json.load(open(f, 'r'))\n",
    "\n",
    "        tokens = data['tokens']\n",
    "        for i in range(len(tokens)):\n",
    "            t = tokens[i]\n",
    "            t[\"attention\"] = [round(float(a), 3) for a in t[\"attention\"]]\n",
    "\n",
    "        json_str = json.dumps(data)\n",
    "        with open(f, \"w\") as o:\n",
    "            o.write(json_str)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "688f53d8-8d2a-4707-9eee-ca91a7dae44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# byLayerHead files\n",
    "for folder in [gpt_folder, bert_folder]:\n",
    "    lh_folder = folder + \"byLayerHead/\"\n",
    "    for json_file in sorted(os.listdir(lh_folder)):\n",
    "        if \".json\" not in json_file:\n",
    "            continue\n",
    "        f = lh_folder + json_file\n",
    "        data = json.load(open(f, 'r'))\n",
    "\n",
    "        tokens = data['tokens']\n",
    "        for i in range(len(tokens)):\n",
    "            t = tokens[i]\n",
    "            for key in t:\n",
    "                t[key] = round(float(t[key]), 3)\n",
    "                \n",
    "        json_str = json.dumps(data)\n",
    "        with open(f, \"w\") as o:\n",
    "            o.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e786e53-dbdb-4c9d-82ed-74d39d480c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-diffusion]",
   "language": "python",
   "name": "conda-env-anaconda3-diffusion-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
