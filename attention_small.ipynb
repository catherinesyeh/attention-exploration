{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7503932-e249-44ff-bc02-3a4822f4e7fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b01d3-1b36-4dd7-a400-9ec462d27c84",
   "metadata": {},
   "source": [
    "Download the Wikipedia dataset + randomly sample sentences as input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f650f-8f49-421b-95bd-e0c08d8b25f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8146d-5e00-4859-9fe8-c12b58b11091",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets # run if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477139f6-d735-4396-ab1d-df09819acac0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b9a0bb-649b-47b7-a51b-fa74bb8284c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: wiki_auto/auto\n",
      "Reusing dataset wiki_auto (/n/home10/cyeh/.cache/huggingface/datasets/wiki_auto/auto/1.0.0/eeac705719dc9aa2ff180571dfed6c6649588ccdfde8d45a47d2e47e5c5b93af)\n",
      "100%|██████████| 2/2 [00:00<00:00, 43.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74894, 4270, 56215, 63250, 75771, 1944, 27013, 60631, 106603, 64395]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset(\"wiki_auto\")\n",
    "\n",
    "# remove data points with no examples\n",
    "data = [dataset['part_1'][i]['normal']['normal_article_content']['normal_sentence'] for i in range(len(dataset['part_1'])) if len(dataset['part_1'][i]['normal']['normal_article_content']['normal_sentence']) != 0] \n",
    "\n",
    "# choose 1000 random data points to sample from\n",
    "data_len = len(data)\n",
    "num_examples = 1000\n",
    "random.seed(10) # set seed\n",
    "indices = random.sample(range(data_len), num_examples)\n",
    "print(indices[:10])\n",
    "\n",
    "# get one sentence for each selected data point\n",
    "sentences = []\n",
    "for i in indices:\n",
    "    # get all sentences associated with data point\n",
    "    sentence_list = data[i]\n",
    "    sentence = random.choice(sentence_list) # choose random sentence from list\n",
    "    sentences.append(sentence)\n",
    "# print(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1886f875-fcdc-4c27-88c2-ed008c9b1a4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Getting Q, K vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1680a5-bafb-46e0-961f-985c9d888d67",
   "metadata": {},
   "source": [
    "Follow Jesse Vig's method of extracting query + key info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9142dbe-1fe2-4a9e-a9ea-ba9899362552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import methods from bertviz\n",
    "from bertviz import neuron_view\n",
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "# parameters\n",
    "model_type = 'bert'\n",
    "model_version = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)\n",
    "\n",
    "sentences_test = sentences[:10] # small sample to test out code with\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "\n",
    "# master dictionary for all values\n",
    "attn_dict = {'left_text': [], \n",
    "             'right_text': [], \n",
    "             'positions': [],\n",
    "             'normalized_positions': [],\n",
    "             'sentences': [],\n",
    "             'tokenized_sentences': [],\n",
    "             'queries': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)], \n",
    "             'keys': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)],\n",
    "             'attn': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)],\n",
    "             'dot_prod': [[[''.join(random.choices(string.ascii_letters, k=5))] for i in range(num_heads)] for j in range(num_layers)]}\n",
    "\n",
    "for s in sentences:\n",
    "    # call method from bertviz to get attention info\n",
    "    s_dict = neuron_view.get_attention(model, model_type, tokenizer, s, include_queries_and_keys=True)['all']\n",
    "    \n",
    "    # append to master dictionary\n",
    "    tokens = s_dict['left_text']\n",
    "    attn_dict['left_text'].extend(tokens)\n",
    "    attn_dict['right_text'].extend(s_dict['right_text'])\n",
    "    \n",
    "    for index in range(len(tokens)): # save position of token and tokenized sentences too\n",
    "        attn_dict['positions'].append(index)\n",
    "        attn_dict['normalized_positions'].append(index / (len(tokens) - 1))\n",
    "        attn_dict['sentences'].append(s)\n",
    "        attn_dict['tokenized_sentences'].append(' '.join(tokens))\n",
    "        \n",
    "    for i in range(num_heads): # updating cumulative q/k vectors + attn + dp\n",
    "        for j in range(num_layers):\n",
    "            q = attn_dict['queries'][i][j]\n",
    "            k = attn_dict['keys'][i][j]\n",
    "            a = attn_dict['attn'][i][j]\n",
    "            d = attn_dict['dot_prod'][i][j]\n",
    "            \n",
    "            if len(q) == 1: # on first round, need to empty list (random string was placeholder)\n",
    "                q.clear()\n",
    "            query = s_dict['queries'][i][j]\n",
    "            q.extend(query)\n",
    "            np_query = np.array(query)\n",
    "            \n",
    "            if len(k) == 1:\n",
    "                k.clear()\n",
    "            key = s_dict['keys'][i][j]\n",
    "            k.extend(key)\n",
    "            np_key = np.array(key)\n",
    "            \n",
    "            if len(a) == 1:\n",
    "                a.clear()\n",
    "            a.extend(s_dict['attn'][i][j])\n",
    "            \n",
    "            if len(d) == 1:\n",
    "                d.clear()\n",
    "            dp = np.dot(np_query, np_key.transpose())\n",
    "            d.extend(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a0b98-1cf4-4f83-810b-a435c1db4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save dictionary to pickle file\n",
    "with open('attn_dict_new.p', 'wb') as file:\n",
    "    pickle.dump(attn_dict, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37147139-7f13-4cd8-8f15-990560a39925",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make TSNE / UMAP Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acb693-8501-4178-840b-7cb51c76be04",
   "metadata": {},
   "source": [
    "Generating plots from query + key vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba3c91-8bb8-4e82-9ced-4582cf1ac251",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221343d8-5f39-436b-9bc3-cd2647214bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc6640-de25-4b00-a711-90e5a73e1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f0bf2-0bef-485c-8d75-42db33505958",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8520c4d-7dd9-4fdc-9b03-94d9174314b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7799895a-ef95-4730-89ca-e1805f748ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ensure plots show up in jupyter\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec11597-843c-442b-874d-e773909a500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load attn_dict back if pre-saved\n",
    "attn_dict = pickle.load( open( \"attn_dict.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0feda50-9cd5-434d-9722-3b1f24e64475",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36034c12-503c-4eef-80a9-1730035feb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce corresponding key matrix from query matrix (e.g., for attention)\n",
    "def k_matrix(q_matrix):\n",
    "    # assumes for specific layer + head (e.g., queries[0][0])\n",
    "    num_tokens = len(q_matrix)\n",
    "    k_matrix = []\n",
    "    i = 0\n",
    "    while i < num_tokens:\n",
    "        q = q_matrix[i]\n",
    "        sent_length = len(q)\n",
    "        for k_i in range(sent_length):\n",
    "            k = []\n",
    "            for q_i in range(sent_length):\n",
    "                k.append(q_matrix[q_i + i][k_i])\n",
    "            k_matrix.append(k)\n",
    "        i += sent_length\n",
    "    \n",
    "    return k_matrix\n",
    "\n",
    "# format sentences to be displayed in html plot\n",
    "def fix_sentences(sentences, positions, types):\n",
    "    new_sentences = []\n",
    "    for sent, pos, t in zip(sentences, positions, types):\n",
    "        s_arr = sent.split()\n",
    "        s = \"\"\n",
    "        for i in range(len(s_arr)):\n",
    "            if i % 10 == 0 and i not in [0, len(s_arr) - 1]:\n",
    "                s += \"<br>\" # add new line every 10 tokens\n",
    "                \n",
    "            if i == pos: # italicize  + color current token\n",
    "                color = \"#B6E1B9\"\n",
    "                if t == \"key\":\n",
    "                    color = \"#F6BA98\"\n",
    "                s += \"<b style='color:\" + color + \"'>\" + s_arr[i] + \"</b>\"\n",
    "            else:\n",
    "                s += s_arr[i]\n",
    "                \n",
    "            if s != len(s_arr) - 1:\n",
    "                s += \" \" # add space back between each token\n",
    "        new_sentences.append(s)\n",
    "    \n",
    "    return new_sentences\n",
    "\n",
    "# convert data into pandas dataframe\n",
    "def make_df(layer, head):\n",
    "    df = pd.DataFrame()\n",
    "    df['token'] = attn_dict['left_text'] + attn_dict['right_text'] # store tokens\n",
    "    df['token'] = df['token'].str.lower() # convert to lowercase\n",
    "    num_tokens = len(attn_dict['left_text'])\n",
    "    \n",
    "    df['type'] = ['query'] * num_tokens + ['key'] * num_tokens # store token type\n",
    "    df['pos_int'] = attn_dict['positions'] * 2 # positions\n",
    "    df['position'] = attn_dict['normalized_positions'] * 2\n",
    "    \n",
    "    # sentence itself\n",
    "    df['sentence'] = fix_sentences(attn_dict['tokenized_sentences'], attn_dict['positions'], df['type'][:num_tokens]) + fix_sentences(attn_dict['tokenized_sentences'], attn_dict['positions'], df['type'][num_tokens:])\n",
    "\n",
    "    # save attn info\n",
    "    attn = attn_dict['attn'][layer][head]\n",
    "    df['attn'] = attn + k_matrix(attn)\n",
    "    dp = attn_dict['dot_prod'][layer][head]\n",
    "    df['dot_prod'] = dp + k_matrix(dp)\n",
    "    \n",
    "    # extract q/k vectors\n",
    "    queries = attn_dict['queries']\n",
    "    keys = attn_dict['keys']\n",
    "    vec_size = len(queries[layer][head][0])\n",
    "    \n",
    "    # norms\n",
    "    norms_q = []\n",
    "    norms_k = []\n",
    "    for i in range(len(queries[layer][head])):\n",
    "        q = queries[layer][head][i]\n",
    "        k = keys[layer][head][i]\n",
    "        norms_q.append(np.linalg.norm(q))\n",
    "        norms_k.append(np.linalg.norm(k))\n",
    "    df[\"norm\"] = norms_q + norms_k\n",
    "\n",
    "    for i in range(vec_size): # store q/k vector values\n",
    "        qs = [queries[layer][head][j][i] for j in range(num_tokens)]\n",
    "        ks = [keys[layer][head][j][i] for j in range(num_tokens)]\n",
    "        df[\"f\" + str(i)] = qs + ks # add to dataframe\n",
    "        \n",
    "    # comment out line below if want all 60k data points\n",
    "    df = pd.concat([df.iloc[:5021], df.iloc[30070:30070+5021]]) # only get first X keys + queries\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c9984a-cf26-4da2-bb4c-3183d41c22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRANSLATING KEYS FOR EASIER COMPARISON\n",
    "def find_q_means(df):\n",
    "    # find mean of each feature in query embeddings\n",
    "    df_queries = df.loc[df['type'] == 'query']\n",
    "    df_queries = df_queries.iloc[:, 8:].copy()\n",
    "    query_means = df_queries.mean(axis=0)\n",
    "    return query_means\n",
    "\n",
    "def find_k_means(df):\n",
    "    # find mean of each feature in key embeddings\n",
    "    df_keys = df.loc[df['type'] == 'key']\n",
    "    df_keys = df_keys.iloc[:, 8:].copy()\n",
    "    key_means = df_keys.mean(axis=0)\n",
    "    return df_keys, key_means\n",
    "\n",
    "def translate_keys(df, df_keys, query_means, key_means):\n",
    "    # translate key vectors accordingly\n",
    "    for i in range(64):\n",
    "        col = \"f\" + str(i)\n",
    "        new_key = df_keys[col] - key_means[col] + query_means[col]\n",
    "        df.loc[df['type'] == 'key', col] = new_key\n",
    "    return df\n",
    "\n",
    "def translate_loop(df): \n",
    "    # whole translation loop\n",
    "    query_means = find_q_means(df)\n",
    "    df_keys, key_means = find_k_means(df)\n",
    "    df = translate_keys(df, df_keys, query_means, key_means)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd4e2102-fdfe-4588-a31f-c7274652b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TSNE AND UMAP\n",
    "def run_tsne(df, layer, head): \n",
    "    # prepare data for feature plot\n",
    "    df_sub = df.iloc[:, 8:].copy()\n",
    "    df_subset = df_sub.values # only get feature cols\n",
    "    \n",
    "    # run TSNE\n",
    "    # from: https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n",
    "    time_start = time.time()\n",
    "    tsne = TSNE(n_components=3, verbose=0, perplexity=100, n_iter=300, metric=\"cosine\") # 3D\n",
    "    # tsne = TSNE(n_components=2, verbose=0, perplexity=100, n_iter=300, metric=\"cosine\") # 2D\n",
    "    tsne_results = tsne.fit_transform(df_subset)\n",
    "    # np.save(\"tsne/layer\" + str(layer) + \"_head\" + str(head) + \".npy\", tsne_results) # save tsne results too\n",
    "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "    \n",
    "    return tsne_results\n",
    "\n",
    "def run_umap(df, layer, head):\n",
    "    # prepare data for feature plot\n",
    "    df_sub = df.iloc[:, 8:].copy()\n",
    "    df_subset = df_sub.values # only get feature cols\n",
    "    \n",
    "    # run umap\n",
    "    time_start = time.time()\n",
    "    umap = UMAP(n_components=3, init='random', random_state=0, metric=\"cosine\")\n",
    "    # umap = UMAP(n_components=2, init='random', random_state=0, metric=\"cosine\") # 2D\n",
    "    umap_results = umap.fit_transform(df_subset)\n",
    "    # np.save(\"umap/layer\" + str(layer) + \"_head\" + str(head) + \".npy\", umap_results) # save umap results too\n",
    "    print('UMAP done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "    \n",
    "    return umap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e6d0df8-eeae-4db7-8526-dda111125af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT GENERATION\n",
    "# add additional columns to df\n",
    "def add_to_df(df, half):\n",
    "    # positions (not normalized)\n",
    "    df['pos_int'] = attn_dict['positions'][:half] * 2\n",
    "    df['pos_int'] = df['pos_int'] + 1\n",
    "    \n",
    "    # length of sentence\n",
    "    words = df['sentence'].str.split().str.len()\n",
    "    df['length'] = words\n",
    "    \n",
    "    # corresponding color for queries/keys\n",
    "    colors = []\n",
    "    for t in df['type']:\n",
    "        if t == \"query\":\n",
    "            colors.append(\"#B6E1B9\")\n",
    "        else:\n",
    "            colors.append(\"#F6BA98\")\n",
    "    df['color'] = colors\n",
    "    df['norm'] = round(df['norm'], 2)\n",
    "\n",
    "    return df\n",
    "\n",
    "def make_fig(tsne_results, df, layer, head, plot_type, half):\n",
    "    # plot TSNE / UMAP results with plotly\n",
    "    # 3D version\n",
    "    # fig = px.scatter_3d(\n",
    "    #     tsne_results[half:], x=0, y=1, z=2,\n",
    "    #     color=df.norm[half:], labels={'color': 'normalized position'}, color_continuous_scale=px.colors.sequential.Burgyl,\n",
    "    #     title=plot_type + ' Plot for BERT (Layer ' + str(layer) + ', Head ' + str(head) + ')',\n",
    "    #     height=800,\n",
    "    #     opacity=0.5\n",
    "    # )\n",
    "    \n",
    "    # 2D version\n",
    "    fig = px.scatter(\n",
    "        tsne_results[half:], x=0, y=1,\n",
    "        color=df.position[half:], labels={'color': 'normalized position'}, color_continuous_scale=px.colors.sequential.Burgyl,\n",
    "        title=plot_type + ' Plot for BERT (Layer ' + str(layer) + ', Head ' + str(head) + ')',\n",
    "        height=800,\n",
    "        opacity=0.5\n",
    "    )\n",
    "    \n",
    "    # 3D version\n",
    "    # fig2 = px.scatter_3d(\n",
    "    #     tsne_results[:half], x=0, y=1, z=2, \n",
    "    #     color=df.position[:half], labels={'color': ''}, color_continuous_scale=px.colors.sequential.Blugrn,\n",
    "    #     title=plot_type + ' Plot for BERT (Layer ' + str(layer) + ', Head ' + str(head) + ')',\n",
    "    #     height=800,\n",
    "    #     opacity=0.5\n",
    "    # )\n",
    "    fig2 = px.scatter(\n",
    "        tsne_results[:half], x=0, y=1, \n",
    "        color=df.position[:half], labels={'color': ''}, color_continuous_scale=px.colors.sequential.Blugrn,\n",
    "        title=plot_type + ' Plot for BERT (Layer ' + str(layer) + ', Head ' + str(head) + ')',\n",
    "        height=800,\n",
    "        opacity=0.5\n",
    "    )\n",
    "    \n",
    "    # add second trace to include 2 color scales (1st is key, 2nd is query)\n",
    "    fig.layout.coloraxis2 = fig2.layout.coloraxis\n",
    "    fig.add_trace(fig2.data[0])\n",
    "    fig['data'][1]['marker'] = {    'color' : df['position'][:half],\n",
    "                                    'coloraxis' : 'coloraxis2',\n",
    "                                    'opacity' : 0.5\n",
    "                                }\n",
    "    # formatting things\n",
    "    fig.layout.coloraxis.colorbar.x = 1.05\n",
    "    fig.layout.coloraxis.colorbar.title.side = \"right\"\n",
    "    fig.layout.coloraxis2.colorbar.x = 1.01\n",
    "    fig.layout.coloraxis2.colorbar.ticklabelstep=70\n",
    "    fig.layout.coloraxis2.colorbar.ticklabelposition=\"inside\"\n",
    "    \n",
    "    # updating display\n",
    "    fig.update_traces( # queries\n",
    "        customdata=df[['token', 'sentence', 'pos_int', 'length', 'type', 'color', 'norm']][:half],\n",
    "        hovertemplate=\"<b style='font-size:larger'><span style='color:%{customdata[5]}'>%{customdata[0]}</span> (<i>%{customdata[4]}</i>, pos: %{customdata[2]} of %{customdata[3]}, norm: %{customdata[6]})</b><br><br>%{customdata[1]}\",\n",
    "        selector=dict(marker_coloraxis='coloraxis2'),\n",
    "        marker=dict(size=6)\n",
    "    )\n",
    "    fig.update_traces( # keys\n",
    "        customdata=df[['token', 'sentence', 'pos_int', 'length', 'type', 'color', 'norm']][half:],\n",
    "        hovertemplate=\"<b style='font-size:larger'><span style='color:%{customdata[5]}'>%{customdata[0]}</span> (<i>%{customdata[4]}</i>, pos: %{customdata[2]} of %{customdata[3]}, norm: %{customdata[6]})</b><br><br>%{customdata[1]}\",\n",
    "        selector=dict(marker_coloraxis='coloraxis'),\n",
    "        marker=dict(size=6)\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='#E8E8E8',\n",
    "        hoverlabel=dict(font_color = 'white', bordercolor = 'white'),\n",
    "    )\n",
    "    \n",
    "    # save plot as html file\n",
    "    # fig.write_html(plot_type + \"_plots/layer\" + str(layer) + \"_head\" + str(head) + \".html\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a1ef8a5-8304-4289-bcc1-dafd60f86335",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FULL TSNE/UMAP LOOPS\n",
    "# generate tsne plot for specific layer, head of BERT\n",
    "def generate_tsne(layer, head):\n",
    "    df = make_df(layer, head)\n",
    "    df = translate_loop(df)\n",
    "    tsne_results = run_tsne(df, layer, head)\n",
    "    half = int(len(tsne_results) / 2)\n",
    "    df = add_to_df(df, half)\n",
    "    make_fig(tsne_results, df, layer, head, \"TSNE\", half)\n",
    "\n",
    "# generate umap plot for specific layer, head of BERT\n",
    "def generate_umap(layer, head):\n",
    "    df = make_df(layer, head)\n",
    "    df = translate_loop(df)\n",
    "    umap_results = run_umap(df, layer, head)\n",
    "    half = int(len(tsne_results) / 2)\n",
    "    df = add_to_df(df, half)\n",
    "    make_fig(umap_results, df, layer, head, \"UMAP\", half)\n",
    "    \n",
    "# generate tsne & umap simultaneously\n",
    "def generate_tsne_and_umap(layer, head):\n",
    "    df = make_df(layer, head)\n",
    "    df = translate_loop(df)\n",
    "    tsne_results = run_tsne(df, layer, head)\n",
    "    umap_results = run_umap(df, layer, head)\n",
    "    half = int(len(tsne_results) / 2)\n",
    "    df = add_to_df(df, half)\n",
    "    make_fig(tsne_results, df, layer, head, \"TSNE\", half)\n",
    "    make_fig(umap_results, df, layer, head, \"UMAP\", half)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f219da-6f2c-4859-a401-98395c641e9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### plot generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f05ce332-f90a-4661-97ac-05b32d6f51e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home10/cyeh/anaconda3/envs/diffusion/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:795: FutureWarning:\n",
      "\n",
      "The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "\n",
      "/n/home10/cyeh/anaconda3/envs/diffusion/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:805: FutureWarning:\n",
      "\n",
      "The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-SNE done! Time elapsed: 72.77628874778748 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"820\"\n",
       "    src=\"iframe_figures/figure_14.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer = 0\n",
    "head = 0\n",
    "\n",
    "# generate single tsne OR umap plot by itself\n",
    "generate_tsne(layer, head)\n",
    "# generate_umap(layer, head)\n",
    "\n",
    "# generate single tsne AND umap plot\n",
    "# generate_tsne_and_umap(layer, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016548a-1c5d-4570-94e9-522da6a051a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for generating plots\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        generate_tsne_and_umap(i, j)\n",
    "        print(\"Layer {} Head {} done\".format(i, j))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-diffusion]",
   "language": "python",
   "name": "conda-env-anaconda3-diffusion-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
